# -*- coding: utf-8 -*-
"""IMDBNaiveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RLDoww_lFpZbG_wITehCTDnwDLcix6oR
"""

import numpy as np
from sklearn import metrics
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from time import time
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
import re
import os
import seaborn as sns
import pandas as pd
from google.colab import drive
drive.mount('/content/gdrive')


df = pd.read_csv("/content/gdrive/My Drive/Machine learning/IMDB.csv")

# df['split'] = np.random.randn(df.shape[0],1)   # them cot split co gia tri theo pp chuan, df.shape[0] = 50000
print(df)
sns.countplot(x='sentiment', data=df)

# Preprocessing


def preprocess_text(sen):
    # Xoa the html
    sentence = remove_tags(sen)

    # Xoa dau cau va so
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Xoa ki tu don
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Xoa nhieu space
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence


tag_re = re.compile(r'<[^>]+>')


def remove_tags(text):
    return tag_re.sub('', text)


x = []  # x[i] chua 1 cmt da duoc tien xu ly
sentences = list(df['review'])
for sen in sentences:
    x.append(preprocess_text(sen))
#print (x[3])

# Chuyen doi label: positive->1, negative->0
y = df['sentiment']
# y[i] chua label duoc chuyen dang so 0,1
y = np.array(list(map(lambda x: 1 if x == "positive" else 0, y)))

# Chia dataset thanh training set and testing set
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.5, random_state=42)
print(x_train[1])
print(y_train)

t = time()
tfidf_vectorizer = TfidfVectorizer()
x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)
duration = time() - t
print("Time taken to extract features from training data : %f seconds" % (duration))
print("n_samples: %d, n_features: %d" % x_train_tfidf.shape)

t = time()
x_test_tfidf = tfidf_vectorizer.transform(x_test)
duration = time() - t
print("Time taken to extract features from test data : %f seconds" % (duration))
print("n_samples: %d, n_features: %d" % x_test_tfidf.shape)

# build naive bayes classification model
t = time()
naive_bayes_classifier = MultinomialNB()

naive_bayes_classifier.fit(x_train_tfidf, y_train)

training_time = time() - t
print("train time: %0.3fs" % training_time)

# predict the new document from the testing dataset
t = time()
y_pred = naive_bayes_classifier.predict(x_test_tfidf)

test_time = time() - t
print("test time:  %0.3fs" % test_time)

# compute the performance measures
score1 = metrics.accuracy_score(y_test, y_pred)
print("accuracy:   %0.3f" % score1)

print(metrics.classification_report(y_test, y_pred,
                                    target_names=['Positive', 'Negative']))

print("confusion matrix:")
print(metrics.confusion_matrix(y_test, y_pred))

print('------------------------------')
